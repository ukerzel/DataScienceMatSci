{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image classification with PyTorch\n",
        "\n",
        "In this exercise we start to build our first (deep learning) model ot analyse images.\n",
        "We use the famous MNIST data that was originally developed by Yann LeCun:  \"THE MNIST DATABASE of handwritten digits\". Yann LeCun, Courant Institute, NYU Corinna Cortes, Google Labs, New York Christopher J.C. Burges, Microsoft Research, Redmond.\n",
        "\n",
        "The dataset contains about 70.000 images of hand-written digits 0-9, 60.000 of which are typically used for training, the rest for testing.\n",
        "The images are normalised to a size of 28x28 pixels and anti-aliased which lead to the grayscale values that we can see in the data (as opposed a pure b/w image).\n",
        "These data have the advantage that they are often included in the tools, and are small enough that we can train a neural network in reasonable time even without using GPUs.\n",
        "\n",
        "A few years ago, the fashion company [Zalando](https://zalando.de) released a variant called [fahion-mnist](https://github.com/zalandoresearch/fashion-mnist) that can be used as a \"drop-in\" replacement, i.e. it has very similar properties: the same number of images for training and testing, the same size in terms of pixels, grayscale images, 10 classes, etc.\n",
        "However, instead of showing just numbers, clothing items are choses (trousers, shirts,etc). The main motivations where that the original MNIST data are too easy for modern systems and can no longer really be used to develop modern algorithms.\n",
        "However, in our case here we stick with the original data, as we want to explore how to setup a system and reduce our depencency on GPUs for now.\n",
        "\n",
        "We will also make use of the [TorchVision](https://pytorch.org/vision/stable/index.html) library. This provides convenient access to a number of datasets (such as MNIST), but also pre-trained models or commonly used image augmentation methods.\n"
      ],
      "metadata": {
        "id": "XerGI0BJSAQH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SbsUiK9_lXNS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision as tv\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data access\n",
        "\n",
        "Now we download the data using the convenience function from TorchVision for the [MNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) data. It is essentially a wrapper function that downloads the data from Yann LeCun's webpage and add it to the local directory."
      ],
      "metadata": {
        "id": "eGyFnO2taTn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = tv.datasets.MNIST(\n",
        "    root=\"data/mnist\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=tv.transforms.ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = tv.datasets.MNIST(\n",
        "    root=\"data/mnist\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=tv.transforms.ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "id": "_9RmXMt0mc4G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to make these data accessible to PyTorch.\n",
        "This can be done using the [DataLoader](https://pytorch.org/docs/stable/data.html) provided by PyTorch. This function provides some common functionality when dealing with the data.\n",
        "In particular, we will need to repeatedly access all images in the dataset and loop over them as the training progresses.\n",
        "Further, experience shows that it is helpful to split the training data into smaller chunks (or batches) to speed up training. This is more efficient than updating the parameters of the network after each single image, or wait until we have processed the entire data in one training loop (or: epoch).\n",
        "Here we will use a batch size of 64 (images) which works well - however, this is a free parameter and differnt values may work better in other situations.\n",
        "\n",
        "After we have defined the DataLoaders for access to the data, we print an example. As we would normally loop over the data, we set the loop up here as well, but terminate it after the first image.\n",
        "\n",
        "The array/vector/tensor ```X``` contains the images. Therfore, it's dimension reflect the batches of images we process:\n",
        "- In each iteration, we train/test the network in batches of 64 images\n",
        "- We have grayscale images (only one colour channel)\n",
        "- the picture is 28x28 pixels high/wide\n",
        "\n",
        "The array/vector/tensor ```y``` contains the corresponding true labels for each image (supervised training).\n",
        "\n",
        "Note the grayscale values due to the anti-aliasing that was applied when creating the data.\n"
      ],
      "metadata": {
        "id": "5-6a5r_obCp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# define the loop over the test data (even if we only use one image)\n",
        "for X, y in test_dataloader:\n",
        "    print(\"Shape of X [N images per batch, # colours, height, width]: \", X.shape)\n",
        "    print(\"Shape of y: \", y.shape, y.dtype)\n",
        "\n",
        "    #print an example\n",
        "    # since PyTorch tensors are of the form [#Channel, #pix X, #pix Y], need to \n",
        "    # transform back into image format\n",
        "    index = 0\n",
        "    plt.imshow(tv.transforms.ToPILImage()(X[index]), cmap='Greys')\n",
        "    print('True label: {}'.format(y[index]) )\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "df3m1KRNnZCl",
        "outputId": "c71efc53-bdd5-402c-80fd-c3ce5c971f91"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N images per batch, # colours, height, width]:  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n",
            "True label: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network definition\n",
        "\n",
        "Before we can train the network, we need to first setup the various components.\n",
        "In particular, we need to define the architecture of the neural network, as well as the optimizer and other parameters that we use to train the network.\n",
        "\n",
        "We start by defining the neural network.\n",
        "In PyTorch, we do so by defining a class inheriting vom ```nn.Module```. We need to do two main steps:\n",
        "- In the ```__init__()``` method: We need to define the components we want to use in our neural network model. Here we can make use of the many building blocks the PyTorch library provides us with and create the respective instances we need.\n",
        "- In the ```forward()``` method: Here we define how the model is built from the various components defined in the ```init()``` method. This defines the *forward* pass of the network (from input to output), hence the name. The backward pass with the update of the network parameters using back-propagation is then handled automatically by PyTorch. Essentially, we need to \"chain\" the components together to define our model: The data (i.e. our images) enter the network through the input layer, go through all intermediate layers and then get to the output layer that provides us, for example, with the predictions for the labels (i.e. here, which number the image shows).\n",
        "\n",
        "However, we also need to make sure that we move the model to GPU if we have one avaiable (together with the data).\n",
        "\n",
        "Generally, when building models working on images we use \n",
        "- [Convolutional](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) layers for learnable image processing \"filters\"\n",
        "- [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layers, e.g. for classification of the labels using all the features learned by the previous (convolutional) layers.\n",
        "- [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) or others as an activation function.\n",
        "- [DropOut](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) or  [Dropout2D](https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html) for regularisation.\n",
        "- [MaxPool2D](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) as the pooling layer\n",
        "\n",
        "When defining the model, we have two hard constraints:\n",
        "- The input layer needs to be able to handle the 28x28x pixel grayscale images\n",
        "- We need 10 nodes in the output layer (one for each digit 0-9).\n",
        "How we set the intermediate layers up is, essentially, up to us and this is where the hard work of building and optimising a model comes in.\n",
        "\n",
        "In a first step, we build a very simple network without convolutional layers. Therefore, we do not expect it to be very good or performant, but it will allow us to \"learn the ropes\" of how to train a neural network.\n",
        "Since we only use \"fully-connected\" (or: \"dense\") layers, we first need to \"flatten\" the 2D image information into a long array that is 28*28 numbers long.\n",
        "This is then passed into the input layer ```fc1```. We choose ```512``` output nodes for this layer. This is passed into the next layer, where we choose also ```512``` ouput nodes, before going to the output layer with ```10``` output nodes. In between, we use ```ReLU``` as activation function.\n",
        "\n",
        "\n",
        "***Exercise*** \\\n",
        "Upgrade the model to use convolutional layers, e.g. two ```Conv2D``` layers with a ```(3,3)``` kernel, ```(1,1)``` stride, as well as 32 and 64 output channels. Then, also use ```maxpool2D``` after the convolutional layers and dropout in the \"dense\" part of the network for regularisation.\n",
        "\n",
        "\n",
        "\n",
        "***Hint*** \\\n",
        "One of the tricky things to evaluate is the output size after pooling layers.\n",
        "This [blog](https://androidkt.com/calculate-output-size-convolutional-pooling-layers-cnn/) has a nice illustrated intuition behind the formulae for convolutional and maxpool layers.\n",
        "\n",
        "For convolutional layers: \\\n",
        "```#output = Floor( ( #input - #filter + 2* #padding)/stride + 1)```\n",
        "\n",
        "For maxpool layers: \\\n",
        "```#output = Floor (  (#input - #filter)/stride +1)```\n",
        "\n",
        "*Note* \\\n",
        "We use the [Functional](https://pytorch.org/docs/stable/nn.functional.html) to pass the data through the various elements, e.g. the activation function."
      ],
      "metadata": {
        "id": "ituN_KKZekHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define  a simple model\n",
        "#class NeuralNetwork(nn.Module):\n",
        "#   def __init__(self):\n",
        "#        super(NeuralNetwork, self).__init__()\n",
        "#        self.flatten = nn.Flatten()\n",
        "#        self.fc1 = nn.Linear(28*28, 512)\n",
        "#        self.fc2 = nn.Linear(512, 512)\n",
        "#        self.fc3 = nn.Linear(512, 10)\n",
        "#        #self.linear_relu_stack = nn.Sequential(\n",
        "#        #    nn.Linear(28*28, 512),\n",
        "#        #    nn.ReLU(),\n",
        "#        #    nn.Linear(512, 512),\n",
        "#        #    nn.ReLU(),\n",
        "#        #    nn.Linear(512, 10)\n",
        "#        #)\n",
        "#\n",
        "#   def forward(self, x):\n",
        "#       x = self.flatten(x)\n",
        "#       #logits = self.linear_relu_stack(x)\n",
        "#       # return logits\n",
        "#       x = self.fc1(x)\n",
        "#       x = F.relu(x)\n",
        "#       x = self.fc2(x)\n",
        "#       x = F.relu(x)\n",
        "#       x = self.fc3(x)\n",
        "#       return x\n",
        "\n",
        "# Define a more complex model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(NeuralNetwork, self).__init__() \n",
        "      self.conv1 = nn.Conv2d(in_channels=1,  out_channels=32, kernel_size= (3,3), stride= (1,1) )\n",
        "      self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size= (3,3), stride= (1,1) )\n",
        "      # calculate output after convolution layer 2:\n",
        "      # #output = Floor( ( #input - #filter + 2* #padding)/stride + 1)\n",
        "      # -> 28-3+1 = 26, one colour channel, 64 convolutional filters\n",
        "      # -> 26 * 26 * 64 * 1\n",
        "      #\n",
        "      # calculate output after MaxPool layer:\n",
        "      # #output = Floor (  (#input - #filter)/stride +1)\n",
        "      #\n",
        "      # here: 1* 64 * ( (26-2)/2) * ( (26-2)/2) = 9216\n",
        "      self.fc1 = nn.Linear(9216, 128)\n",
        "      self.fc2 = nn.Linear(128, 10)\n",
        "      self.dropout1 = nn.Dropout2d(0.25)\n",
        "      self.dropout2 = nn.Dropout2d(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = F. relu(x)\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "      x = F.max_pool2d(x, kernel_size=(2,2), stride=(2,2))\n",
        "      x = torch.flatten(x,1)\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.dropout2(x)\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RJBMA0fnpKc",
        "outputId": "f082448e-d863-4709-9c84-cd73d7df7a02"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
            "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to define the further parameters of the network.\n",
        "Two very important ingredients are:\n",
        "- the optimiser that is used to adjust the network weights during training\n",
        "- the loss function \n",
        "\n",
        "The loss function allows us to quantify how well the network performs and how \"good\" the predictions are, i.e. how close the predicted label is to the true label. Since we work on a classification task, we use the [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "\n",
        "Instead of stochastic gradient descent [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html), we will use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html?highlight=adam#torch.optim.Adam) optimiser, which is more efficient. We could also use, e.g. [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html?highlight=adam#torch.optim.AdamW) as a modern variant.\n",
        "We need to pass the model parameters (i.e. the network weights) to the optimiser. This essentially \"tells\" the optimiser, which parameters need to be tuned during training."
      ],
      "metadata": {
        "id": "dfa4uLhxtFTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "## Optimizer and Loss Function\n",
        "##\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Choose the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "_DSpxVO6n52R"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network training\n",
        "\n",
        "Here we define the functions that run over the data and either train the network or evaluate the test data\n",
        "\n",
        "The main skelleton is the same:\n",
        "- we loop over batches over the data\n",
        "- move the data to the device (CPU or GPU)\n",
        "- calculate the predictions with the current state of the network (i.e., a forward pass)\n",
        "- determine the quality of the prediction (i.e. the value of the loss function).\n",
        "\n",
        "For the training loop, we then need to tell PyTorch to do the backward propagation and update the network weights.\n",
        "\n",
        "Note:\n",
        "- We need to tell PyTorch explictly if we train the model, so that the weights can get updated: ```model.train()```. Otherwise, we switch to evaluation mode ```model.eval()```.\n",
        "- Our network has 10 output nodes, i.e. each note tells us how likely it is that this image corresponds to the label represented by this node. We need the node with the highest value to identify the most likely classification which we can do with ```argmax```."
      ],
      "metadata": {
        "id": "Mc3GlViTuv9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Training loop\n",
        "#\n",
        "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # put the model into training mode\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # need to send the data tensors explicitly to the device we want to \n",
        "        # train on. E.g. when we create the tensors (and load the data),\n",
        "        # they get created on the CPU, if we want to train on the GPU, \n",
        "        # we need to move it there explictly.\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        #\n",
        "        # Forward pass - compute the output of the network\n",
        "        # (do not call forward() directly), \"apply\" the model on the data,\n",
        "        # compute prediction error\n",
        "        #\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        #\n",
        "        # Backpropagation\n",
        "        #\n",
        "\n",
        "        # By default, the gradients accumulate, so we need to \n",
        "        # reset them explicitly at each new training step\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Now we need to calculate the gradients in all layers\n",
        "        # in the backward pass. \n",
        "        # (For some reason, this is method is owned by the loss function)\n",
        "        loss.backward()\n",
        "\n",
        "        # After we calculated the gradients, we need to take the step\n",
        "        # in the next best direction.\n",
        "        # (For some reason, this method is owned by the optimizer)\n",
        "        optimizer.step()\n",
        "\n",
        "        # record loss functions // change loss -> loss_item ??\n",
        "        loss_item = loss.item()\n",
        "        current = batch * len(X)\n",
        "\n",
        "        running_loss += loss_item\n",
        "        if batch % 100 == 0:\n",
        "            #loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss_item:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return running_loss/size\n",
        "\n",
        "\n",
        "#\n",
        "# Test Loop\n",
        "#\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    # put the model into the evaluation mode.\n",
        "    # some aspects (e.g. dropout, etc.) behave differently during\n",
        "    # trainig and inference/evaluation\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # PyTorch stores gradient information by default - \n",
        "    # during inference we no longer need this as we won't update the\n",
        "    # parameters anymore. Therefore, we can use \"no_grad()\" to switch that off.\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\") \n",
        "    return correct"
      ],
      "metadata": {
        "id": "y2bcXu2DoC3L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use these functions for the actual training."
      ],
      "metadata": {
        "id": "vANwQ3QFwMMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "## The actual training/evaluation\n",
        "##\n",
        "epochs = 5\n",
        "loss_values = []\n",
        "accuracy_values = []\n",
        "\n",
        "for t in range(epochs):\n",
        "\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    loss = train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
        "    loss_values.append(loss)\n",
        "\n",
        "    accuracy = test(test_dataloader, model, loss_fn)\n",
        "    accuracy_values.append(accuracy)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh-MwE-PonOb",
        "outputId": "ec49db09-a97d-4386-9da9-22b1463d466d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.302431  [    0/60000]\n",
            "loss: 0.516572  [ 6400/60000]\n",
            "loss: 0.209629  [12800/60000]\n",
            "loss: 0.256616  [19200/60000]\n",
            "loss: 0.150995  [25600/60000]\n",
            "loss: 0.273445  [32000/60000]\n",
            "loss: 0.125058  [38400/60000]\n",
            "loss: 0.194553  [44800/60000]\n",
            "loss: 0.173202  [51200/60000]\n",
            "loss: 0.054062  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.2%, Avg loss: 0.057974 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.075864  [    0/60000]\n",
            "loss: 0.162177  [ 6400/60000]\n",
            "loss: 0.062753  [12800/60000]\n",
            "loss: 0.211737  [19200/60000]\n",
            "loss: 0.040378  [25600/60000]\n",
            "loss: 0.113958  [32000/60000]\n",
            "loss: 0.186486  [38400/60000]\n",
            "loss: 0.063926  [44800/60000]\n",
            "loss: 0.111351  [51200/60000]\n",
            "loss: 0.126733  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.037784 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.020807  [    0/60000]\n",
            "loss: 0.156965  [ 6400/60000]\n",
            "loss: 0.205388  [12800/60000]\n",
            "loss: 0.081414  [19200/60000]\n",
            "loss: 0.079119  [25600/60000]\n",
            "loss: 0.073816  [32000/60000]\n",
            "loss: 0.110140  [38400/60000]\n",
            "loss: 0.084735  [44800/60000]\n",
            "loss: 0.152536  [51200/60000]\n",
            "loss: 0.158382  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.7%, Avg loss: 0.037262 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.077554  [    0/60000]\n",
            "loss: 0.046209  [ 6400/60000]\n",
            "loss: 0.022607  [12800/60000]\n",
            "loss: 0.180886  [19200/60000]\n",
            "loss: 0.025648  [25600/60000]\n",
            "loss: 0.083001  [32000/60000]\n",
            "loss: 0.052796  [38400/60000]\n",
            "loss: 0.099216  [44800/60000]\n",
            "loss: 0.096892  [51200/60000]\n",
            "loss: 0.089153  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.034399 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.012141  [    0/60000]\n",
            "loss: 0.036472  [ 6400/60000]\n",
            "loss: 0.029155  [12800/60000]\n",
            "loss: 0.084207  [19200/60000]\n",
            "loss: 0.113476  [25600/60000]\n",
            "loss: 0.070855  [32000/60000]\n",
            "loss: 0.082783  [38400/60000]\n",
            "loss: 0.028141  [44800/60000]\n",
            "loss: 0.117540  [51200/60000]\n",
            "loss: 0.063870  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 99.0%, Avg loss: 0.033949 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, sharex=True)\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].plot(loss_values)\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Iteration\", fontsize=14)\n",
        "axes[1].plot(accuracy_values)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "TxwSKJM31JdH",
        "outputId": "3c86b053-47cb-4d91-b826-d8d58273f378"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even with the very simple fully connected network, we can achive an accuracy of 96%-97%. This isn't bad as such, indeed, in many applications, this would be considered phenomenal. However, the best classifiers achieve around 99.8%, the code at [MNIST-0.17](https://github.com/Matuzas77/MNIST-0.17) has an error rate of only 0.17%.\n",
        "This illustrates why we use MNIST for learning how to use neural networks for image processing and how to set the system up - but the data are not suitable (anymore) to develop performant neural network architectures."
      ],
      "metadata": {
        "id": "NJ_rwY8vxAom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The network with a few convolutional neural entwork layers is much better than the one without - we achieve an accuracy of 99% after just 5 epochs of training.\n",
        "This is still quite far from the current state of the art, but then again, we do not want to build a model to beat it.\n",
        "However, feel free to try and improve the model."
      ],
      "metadata": {
        "id": "Hrh_VH-swZgF"
      }
    }
  ]
}